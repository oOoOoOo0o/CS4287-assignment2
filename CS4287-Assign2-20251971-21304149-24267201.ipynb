{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Bayan Nezamabad 20251971<br>\n",
    "Jacob Beck 21304149<br>\n",
    "Ella Hirche 24267201\n",
    "\n",
    "Code executes to the end without errors\n",
    "\n",
    "### References:\n",
    "[The dataset](https://www.kaggle.com/datasets/shahidulugvcse/national-flowers) <br>\n",
    "[Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842) <br>\n",
    "[Implementation of GoogLeNet on Keras](https://lekhuyen.medium.com/implementation-of-googlenet-on-keras-d9873aeed83c) <br>\n",
    "[TensorFlow Tutorials](https://www.tensorflow.org/tutorials) <br>\n",
    "[TensorFlow Docs](https://www.tensorflow.org/api_docs) <br>\n",
    "[Matplotlib Docs](https://matplotlib.org/stable/)"
   ],
   "id": "9ec14d6502d8ea28"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imports\n",
    "import kagglehub\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras import utils, Model\n",
    "from keras.api.layers import Dense, Dropout, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Concatenate, Input, Flatten\n",
    "from keras.api.initializers import HeNormal\n",
    "from keras.api.optimizers import SGD, RMSprop, Adam, Lion\n",
    "from keras.api.callbacks import EarlyStopping\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mpimg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Downloading dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"shahidulugvcse/national-flowers\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "id": "749fb9318116f25c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The data set\n",
    "Our dataset consists of images of 9 different types of national flowers which we will be classifying. Various features can be extracted from patterns in our data such as petals, pistils, stems, etc. <br>\n",
    "The dataset contains images where the flowers are of various sizes, not always centered, part of the flower may be cropped out, images may have borders, images may contain multiple of the flower. <br>\n",
    "The dataset has two directories, consisting of 4481 files, split roughly 80% in the training directory and 20% in the testing directory. <br>\n",
    "The images are of varying sizes, so we will have to do some preprocessing to make them of uniform size. <br>\n",
    "\n",
    "### Data preprocessing\n",
    "Upon loading the dataset, some preprocessing is performed to make all images 224x224 since that is the requisite input size for GoogLeNet. <br>\n",
    "The images are also <b>padded</b> to retain the same aspect ratio. Since the features are not always around the center of the image, we chose padding over cropping since we don't want to accidentally crop out valuable features which may have an adverse effect on training."
   ],
   "id": "625d20d1a0d7cd11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loading the data set\n",
    "# The train directory contains directories for each class/flower\n",
    "train_dir = path + '\\\\flowerdataset\\\\train'\n",
    "# The test directory contains directories for each class/flower\n",
    "test_dir = path + '\\\\flowerdataset\\\\test'\n",
    "\n",
    "# Some configurations\n",
    "input_shape = (224, 224)\n",
    "batch_size = 128\n",
    "epochs = 50 # Initial estimate that will be refined later\n",
    "\n",
    "verbose = True\n",
    "class_names = os.listdir(train_dir) # Getting class names based on directory names as each class' data is in a separate directory\n",
    "NB_CLASSES = len(class_names)\n",
    "\n",
    "# Defining a function for loading datasets\n",
    "# By default it loads a full dataset but you can specify the split and which part of the split you want\n",
    "def load_dataset(directory, validation_split=0.0, subset='training'):\n",
    "    if validation_split <= 0.0:\n",
    "        return utils.image_dataset_from_directory(\n",
    "            directory=directory,\n",
    "            label_mode='categorical',\n",
    "            image_size=input_shape, # Images are resized to the correct dimensions\n",
    "            pad_to_aspect_ratio=True, # Padding images so as not to lose valuable features & preserve aspect ratio\n",
    "            seed=123, # Use a seed for reproducibility\n",
    "            verbose=verbose)\n",
    "    else:\n",
    "        return utils.image_dataset_from_directory(\n",
    "            directory=directory,\n",
    "            label_mode='categorical',\n",
    "            image_size=input_shape, # Images are resized to the correct dimensions\n",
    "            pad_to_aspect_ratio=True, # Padding images so as not to lose valuable features & preserve aspect ratio\n",
    "            validation_split=validation_split,\n",
    "            subset=subset,\n",
    "            seed=123, # Use a seed for reproducibility\n",
    "            verbose=verbose)\n",
    "\n",
    "# Loading our training dataset using a 0.2 split and the training subset means we will take the 80% part\n",
    "train_ds = load_dataset(train_dir, validation_split=0.2, subset='training')\n",
    "\n",
    "# Loading the remainder of our training dataset as our validation dataset\n",
    "valid_ds = load_dataset(train_dir, validation_split=0.2, subset='validation')\n",
    "\n",
    "# Loading our testing dataset, by excluding a specified validation split we take the whole dataset\n",
    "test_ds = load_dataset(test_dir)\n",
    "\n",
    "# Extracting test images and labels from the dataset\n",
    "X_test = [] # Array of images\n",
    "y_test = [] # Array of labels corresponding to the images\n",
    "for x_t, y_t in test_ds:\n",
    "    for i in range(x_t.shape[0]):\n",
    "        X_test.append(x_t[i].numpy())\n",
    "        y_test.append(y_t[i].numpy())\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ],
   "id": "e7df28aead6330b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "An example of padding vs cropping on a sample image is shown below to illustrate how cropping can drastically alter the quality of our training dataset by removing a lot of relevant features. <br>\n",
    "The left image is unaltered, the center image is padded maintaining aspect ratio, and the right image is cropped maintaining aspect ratio. <br>\n",
    "As can be seen, the cropped image gets rid of almost the entire flower while the padded image retains all the features of the flower. <br><br>\n",
    "It should be noted however, that padding does generate black borders around images that need padding which the CNN will have to learn to ignore as it shouldn't contribute to classification."
   ],
   "id": "db3b25d14d265453"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Padding vs cropping\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title('Unaltered image vs Padded image vs Cropped image')\n",
    "plt.axis('off')\n",
    "# Defining a path to an image that illustrates the difference between padding and cropping\n",
    "image_path = train_dir + '\\\\Lilly\\\\334c1f79fd.jpg'\n",
    "\n",
    "# Displaying unaltered image on the left\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "img = mpimg.imread(image_path) / 255.0 # Read the image and normalise so it can be displayed properly with pyplot\n",
    "plt.imshow(img)\n",
    "\n",
    "# Displaying padded image in the centre\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "img = tf.convert_to_tensor(img, dtype=tf.float32)  # resize_with_pad takes a tensor so we convert the image into one here\n",
    "img = tf.image.resize_with_pad(img, 224, 224)  # generates black (0) padding where needed\n",
    "plt.imshow(img.numpy())\n",
    "\n",
    "# Displaying cropped image on the right\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "img = utils.load_img(image_path, target_size=input_shape, keep_aspect_ratio=True)  # keep_aspect_ratio crops the image from the center\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ],
   "id": "f83801f5d87e7e6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "The next preprocessing step is <b>normalisation</b>. We will use min-max normalisation to make all pixel values from the range 0 - 255 to 0 - 1."
   ],
   "id": "cc0021d156bc397e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Min-max normalisation\n",
    "# x represents image data so we divide it by 255 to normalise each image's pixel values\n",
    "train_ds = train_ds.map(lambda x, y: (x / 255.0, y))\n",
    "valid_ds = valid_ds.map(lambda x, y: (x / 255.0, y))\n",
    "X_test /= 255.0"
   ],
   "id": "fc3e68e2c7d92351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data visualisation\n",
    "\n",
    "Here we will visualise some of the data.<br>\n",
    "First we will display a <b>sample image of each class</b> from the training dataset.\n",
    "The images contain varying backgrounds and are of varying dimensions. But in our datasets we have scaled them all down to 224x224 and zero-padded them for use with our models.<br><br>\n",
    "Next we will display the <b>class distribution</b> for the training dataset and the testing dataset. As can be seen, the training dataset is entirely evenly distributed, with each class having 400 training images. 80 images of each class are being reserved for validation in accordance with our 80/20 training/validation split. <br>\n",
    "The testing dataset is also evenly distributed for the most part. The only minor difference is that there are 92 test images for the Daisy class and 89 for the Orchid class. Every other class has 100 test images each.\n",
    "\n"
   ],
   "id": "238b44117ed2f1f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Data visualisation\n",
    "\n",
    "# Displaying sample images from the training dataset on a 3x3 grid\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "# Flattening axes to easily loop through them\n",
    "axes = axes.flatten()\n",
    "# For each class name\n",
    "for i, class_name in enumerate(class_names):\n",
    "    image_file = os.listdir(train_dir + '\\\\' + class_name)[0] # This will get the first image filename in each class\n",
    "    image_path = train_dir + '\\\\' + class_name + '\\\\' + image_file # Constructing the full path to the image\n",
    "    img = mpimg.imread(image_path)\n",
    "    axes[i].imshow(img) # Show each image\n",
    "    axes[i].set_title(class_name) # Display the type of flower it is\n",
    "    axes[i].set_axis_off()\n",
    "\n",
    "# Visualising class distribution for the training dataset and the testing dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# This array stores the number of images per class\n",
    "class_distribution = []\n",
    "for name in class_names:\n",
    "    class_dir = os.path.join(train_dir, name) # This gets the path to each classes' directory\n",
    "    class_distribution.append(len(os.listdir(class_dir)))  # This gives us the length of the array of files in class_dir\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "# Displays a bar chart of the class distribution with alternating colours\n",
    "ax.bar(class_names, class_distribution, color=['#71b8d9', '#4686a3'])\n",
    "ax.set_title('Training Dataset Class Distribution')\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Number of Images')\n",
    "# Rotating x ticks for readability\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "# Now we do the same with the testing dataset\n",
    "class_distribution = []\n",
    "for name in class_names:\n",
    "    class_dir = os.path.join(test_dir, name)\n",
    "    class_distribution.append(len(os.listdir(class_dir)))\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.bar(class_names, class_distribution, color=['#71b8d9', '#4686a3'])\n",
    "ax.set_title('Test Dataset Class Distribution')\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Number of Images')\n",
    "ax.tick_params(axis='x', labelrotation=90)"
   ],
   "id": "6ea2f285c6d56560",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The network structure and hyperparameters\n",
    "\n",
    "### GoogLeNet\n",
    "Below are diagrams representing the architecture of GoogLeNet. <br>\n",
    "![googlenet-architecture](https://drive.google.com/uc?id=1tWUNza79fQnQ-aJm-tc4hZw-aHEVCD5R) <br>\n",
    "<i>GoogLeNet architecture</i> <br>\n",
    "![inception-module](https://drive.google.com/uc?id=1RDRB1u-p2nup2MeYcCS9TkyXSbC_t6Qr) <br>\n",
    "<i>Inception module</i> <br>\n",
    "![auxiliary-classifier](https://drive.google.com/uc?id=145__39f0Z28chCQF0dGZ6IfeQKQt3Nvj) <br>\n",
    "<i>Auxiliary classifier</i> <br><br>\n",
    "GoogLeNet is a complex model that notably uses significantly less parameters than AlexNet through the use of inception modules. <br>\n",
    "Inception modules have four paths that can capture different features which are then concatenated at the end. 1x1 convolutions are used to reduce the number of channels in the feature maps and to assist in deeper convolutions using ReLU activation. Then other operations can be performed, namely 3x3 convolution, 5x5 convolution and max-pooling which all serve to capture different patterns or features in the data. The convolutions of varying sizes can capture patterns at different scales while the pooling emphasises the most significant pixels in a region. All these operations use same-padding to retain their resolution as they are concatenated at the end, which essentially puts all their channels together, retaining all the information gleaned from these paths. For example if the 4 paths have a total of 50 channels combined, the output of concatenation would be their height x width x 50. <br>\n",
    "An inception network such as GoogLeNet uses various inception modules often reducing their resolution as it gets deeper through the use of pooling with a stride greater than 1, often 2 to halve the resolution. This type of network allows for high performance with lower computational demands. The model we define in the below code cell has ~10,000,000 parameters, whereas AlexNet may have over 6 times that - a massive reduction in parameters thanks to inception modules. This design can also be considered intuitive as it aggregates visual information processed at different scales simultaneously meaning a richer set of features are learned. <br><br>\n",
    "The GoogLeNet architecture makes use of inception modules to form a complex CNN alongside a few convolutional layers and pooling to reduce resolution as we go deeper. The input size is 224x224x3. While the model is about 27 layers deep the total number of layers is over 4 times the size since inception modules have simultaneous paths. We have added batch normalization after the convolutions with larger receptive fields in the architecture (outside of inception modules) to help with convergence. The final layers use average pooling to effectively flatten the feature map by getting the average value of the 7x7 input field which had resulted in improved accuracy for the authors of GoogLeNet. Lastly 40% dropout is used as a form of regularisation that reduces overfitting by forcing the model to learn a robust and complete set of features. Then lastly a fully-connected layer using softmax and 9 units generates our one-hot encoded output. <br><br>\n",
    "Since the network is deep, there could be issues with backpropagating gradients. Auxiliary classifiers were introduced to tackle this and mitigate the effects of vanishing/exploding gradients. An auxiliary classifier are placed after the third and sixth inception modules since even at this point in the network there are some valuable features gleaned. This manages to ensure the gradient is propagated effectively while also not compromising classification performance. These auxiliary classifiers' losses are weighted at 0.3 of the main loss each to balance the training process. <br>\n",
    "The auxiliary classifier consists of an average pooling layer, 1x1 convolutional layer, then flattens the feature map for a fully connected layer with 1024 units and 70% dropout is performed before the output fully connected layer. We have altered this to 50% as 70% seemed to be too high hindering our learning. <br><br>\n",
    "He Normal weight initialisation is used since it is designed to improve performance for networks using ReLU activation which GoogLeNet does. Since ReLU can lead to sparse activations as all neurons 0 and below output only 0, so only positive neurons are active. He Normal initialisation improves the flow of gradient by initialising weights based on a normal distribution with mean of 0 and standard deviation of the root of 2 over the number of input neurons for the layer. This leads to higher variance which reduces the impact of sparse activation while also avoiding the other extreme of exploding gradients. <br><br>\n",
    "The code block below also contains some preparatory code for fine-tuning our configuration which is explained in further depth under the <b>Optimisers</b> section. <br>\n",
    "Our initial epoch value is 50 but we will see if so many epochs are needed by using a early stopping callback when we are no longer bettering our performance in training. This is explained more in later code blocks."
   ],
   "id": "fbdca9eb962865f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This is a helper function that creates an optimiser with a specified learning rate (LR)\n",
    "# This is used since we want to find the best combination of optimiser and LR\n",
    "# As can be seen, we will be testing 4 optimisers: SGD, RMSProp, Adam, and Lion\n",
    "def make_optimiser(optimiser, learning_rate):\n",
    "    match(optimiser):\n",
    "        case 'sgd':\n",
    "            return SGD(learning_rate=learning_rate)\n",
    "        case 'rmsprop':\n",
    "            return RMSprop(learning_rate=learning_rate)\n",
    "        case 'adam':\n",
    "            return Adam(learning_rate=learning_rate)\n",
    "        case 'lion':\n",
    "            return Lion(learning_rate=learning_rate)\n",
    "\n",
    "# Full GoogleNet architecture\n",
    "\n",
    "# A helper function to easily define inception modules\n",
    "def build_inception_module(input_layer, filters):\n",
    "    # First path with 1 1x1 convolutional layer\n",
    "    path1 = Conv2D(filters=filters[0], kernel_size=(1, 1), padding='same', activation='relu', kernel_initializer=HeNormal())(input_layer)\n",
    "\n",
    "    # Second path using a 3x3 convolutional layer with a 1x1 to reduce dimensionality\n",
    "    path2 = Conv2D(filters=filters[1][0], kernel_size=(1, 1), padding='same', activation='relu', kernel_initializer=HeNormal())(input_layer)\n",
    "    path2 = Conv2D(filters=filters[1][1], kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer=HeNormal())(path2)\n",
    "\n",
    "    # Third path using a 5x5 convolutional layer with a 1x1 to reduce dimensionality\n",
    "    path3 = Conv2D(filters=filters[2][0], kernel_size=(1, 1), padding='same', activation='relu', kernel_initializer=HeNormal())(input_layer)\n",
    "    path3 = Conv2D(filters=filters[2][1], kernel_size=(5, 5), padding='same', activation='relu', kernel_initializer=HeNormal())(path3)\n",
    "\n",
    "    # Fourth path using max pooling wih a 3x3 pool size to extract the most significant pixels followed by a 1x1 convolutional layer\n",
    "    path4 = MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding='same')(input_layer)\n",
    "    path4 = Conv2D(filters=filters[3], kernel_size=(1, 1), padding='same', activation='relu', kernel_initializer=HeNormal())(path4)\n",
    "\n",
    "    # Here we concatenate the layers into one layer with depth of the sum of the 4 paths.\n",
    "    # So the depth of the concatenated layer will be filters[0] + filters[1][1] + filters[2][1] + filters[3]\n",
    "    inception_layer = Concatenate(axis=-1)([path1, path2, path3, path4])\n",
    "    return inception_layer\n",
    "\n",
    "# A helper function to easily define auxiliary classifiers\n",
    "def build_auxiliary_classifier(input_layer, name):\n",
    "    # 5x5 average pooling with stride of 3 reducing the resolution by a third of the original\n",
    "    aux = AvgPool2D(pool_size=(5, 5), strides=3, padding='valid')(input_layer)\n",
    "    # A 1x1 convolutional layer to reduce dimensionality\n",
    "    aux = Conv2D(filters=128, kernel_size=(1, 1), padding='same', activation='relu', kernel_initializer=HeNormal())(aux)\n",
    "    # Flattening for our fully connected layers\n",
    "    aux = Flatten()(aux)\n",
    "    # A fully connected layer with 1024 units using ReLU activation\n",
    "    aux = Dense(units=1024, activation='relu', kernel_initializer=HeNormal())(aux)\n",
    "    # 50% dropout (lowered from the 70% in the original GoogLeNet)\n",
    "    aux = Dropout(rate=0.5)(aux)\n",
    "    # Output using softmax and one-hot encoding\n",
    "    aux = Dense(units=NB_CLASSES, activation='softmax', name=name)(aux)\n",
    "\n",
    "    return aux\n",
    "\n",
    "# This function builds all layers in the GoogLeNet architecture\n",
    "def build_googlenet_model(tuner = None):\n",
    "    # Input images are 224x224 with 3 channels for RGB\n",
    "    input_layer = Input(shape=(224, 224, 3))\n",
    "\n",
    "    # First some initial larger convolution is performed with a stride of 2, detecting larger patterns and halving our resolution\n",
    "    layer = Conv2D(filters=64, kernel_size=(7, 7), strides=2, padding='same', activation='relu', kernel_initializer=HeNormal())(input_layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    # Again we simplify the input further with a max pooling of stride 2\n",
    "    layer = MaxPool2D(pool_size=(3, 3), strides=2, padding='same')(layer)\n",
    "\n",
    "    # Smaller convolution is performed here with a 1x1 and 3x3\n",
    "    layer = Conv2D(filters=64, kernel_size=(1, 1), strides=1, padding='same', activation='relu', kernel_initializer=HeNormal())(layer)\n",
    "    layer = Conv2D(filters=192, kernel_size=(3, 3), strides=1, padding='same', activation='relu', kernel_initializer=HeNormal())(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    # The resolution is reduced once more before the inception modules\n",
    "    layer = MaxPool2D(pool_size=(3, 3), strides=2, padding='same')(layer)\n",
    "\n",
    "    # 2 inception modules are used with different filter number configurations\n",
    "    layer = build_inception_module(layer, [64, (96, 128), [16, 32], 32])\n",
    "    layer = build_inception_module(layer, [128, (128, 192), [32, 96], 64])\n",
    "\n",
    "    # Halving the resolution again, at this point the dimensions are 14x14x480\n",
    "    layer = MaxPool2D(pool_size=(3, 3), strides=2, padding='same')(layer)\n",
    "\n",
    "    layer = build_inception_module(layer, [192, (96, 208), [16, 48], 64])\n",
    "    # Our first auxiliary classifier is here and takes the feature maps from the last inception module (14x14x512)\n",
    "    # This will help backpropagation of gradient to the previous layers\n",
    "    auxiliary1 = build_auxiliary_classifier(layer, 'aux1')\n",
    "    # Three more inception modules\n",
    "    layer = build_inception_module(layer, [160, (112, 224), [24, 64], 64])\n",
    "    layer = build_inception_module(layer, [128, (128, 256), [24, 64], 64])\n",
    "    layer = build_inception_module(layer, [112, (144, 288), [32, 64], 64])\n",
    "    # The second auxiliary classifier takes an input of 14x14x528\n",
    "    auxiliary2 = build_auxiliary_classifier(layer, 'aux2')\n",
    "    layer = build_inception_module(layer, [256, (160, 320), [32, 128], 128])\n",
    "\n",
    "    # At this point we halve the resolution again using max pooling. Size is now 7x7x832\n",
    "    layer = MaxPool2D(pool_size=(3, 3), strides=2, padding='same')(layer)\n",
    "\n",
    "    layer = build_inception_module(layer, [256, (160, 320), (32, 128), 128])\n",
    "    # After this final inception module we will have 1024 7x7 feature maps\n",
    "    layer = build_inception_module(layer, [384, (192, 384), (48, 128), 128])\n",
    "\n",
    "    # This gets the average of the 7x7 field for each feature map resulting in 1x1x1024\n",
    "    layer = AvgPool2D(pool_size=(7, 7), strides=1, padding='valid')(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    # 40% dropout for robust learning\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    # Finally we output our final one-hot encoded classification with softmax\n",
    "    output_layer = Dense(units=NB_CLASSES, activation='softmax', name='output')(layer)\n",
    "\n",
    "    # Now we create our model making sure to use the three outputs - the two auxiliary layers and the final output layer\n",
    "    model = Model(inputs=input_layer, outputs=[output_layer, auxiliary1, auxiliary2], name='GoogLeNet')\n",
    "    \n",
    "    # If a tuner is specified we will use this code block to configure our optimiser & LR\n",
    "    # Otherwise we skip this step and the model is compiled manually\n",
    "    if tuner is not None:\n",
    "        # These two choices give us 4x4 = 16 different combinations to test\n",
    "        learning_rate = tuner.Choice('learning_rate', [0.001, 0.0001, 0.00005, 0.00001])\n",
    "        optimiser_choice = tuner.Choice('optimiser', ['sgd', 'rmsprop', 'adam', 'lion'])\n",
    "    \n",
    "        # Here we initialiser out optimiser using the results from the choices\n",
    "        optimiser = make_optimiser(optimiser_choice, learning_rate)\n",
    "    \n",
    "        # We compile our model with the optimiser\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['accuracy', 'accuracy', 'accuracy'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Building a sample model to display a summary of the architecture                                               \n",
    "googlenet = build_googlenet_model()\n",
    "googlenet.summary()"
   ],
   "id": "f5010c9b62563093",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loss Function\n",
    "Since both models use one-hot encoding we chose to go with <b>categorical crossentropy</b> (CCE) which is best for one-hot encoded classification tasks. <br>\n",
    "What this does is measures the sum of the probabilities assigned to incorrect classes since each incorrect class should be assigned a probability of 0, while the correct class should be assigned a probability of 1. <br>\n",
    "Sparse categorical crossentropy is a similar loss function that achieves the same goal but for output encoded using numeric/ordinal encoding. <br>\n",
    "Other loss functions such as root mean squared error or mean absolute error are better suited for regression tasks so we avoid using them here.\n",
    "## Optimiser\n",
    "### GoogLeNet Optimiser and Learning Rate Selection\n",
    "There were four main optimisers that we investigated with our GoogLeNet architecture: Stochastic Gradient Descent (SGD), RMSProp, Adam, and Lion. <br>\n",
    "- SGD was expected to be the least effective but serves as a baseline and also to test a gradient descent optimiser. Since our dataset has padding and noisy backgrounds it may take a long time to converge.\n",
    "- RMSProp uses an adaptive learning rate and uses a moving average of squared gradients to adjust the learning rate.\n",
    "- Adam is one of the most commonly used optimisers. It's similar to RMSProp in that it uses moving average as well but also considers momentum, the direction the gradients are moving in.\n",
    "- Lion is a newer optimiser discovered by Google Brain. It considers momentum like Adam, but uses the sign operation making each parameter updated by the same magnitude. This makes it memory efficient and supposedly outperforms Adam.\n",
    "While it seems that Lion may be the best optimiser, we will test each optimiser with learning rates: 0.001, 0.0001, 0.00005, and 0.00001 using Hyperband. Hyperband uses brackets to determine the best performing model, taking the best 50% of models to the next bracket. By putting models with each optimiser with each learning rate into a Hyperband tuner we can determine the best pair in a reasonable amount of time. We are comparing our models on validation loss."
   ],
   "id": "928f64a636994837"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiating our tuner to find the best performing optimiser and learning rate (LR) pair based on validation loss\n",
    "tuner = kt.Hyperband(build_googlenet_model,\n",
    "                     objective='val_loss',\n",
    "                     max_epochs=epochs,\n",
    "                     directory='googlenet',\n",
    "                     project_name='GoogLeNet')\n",
    "\n",
    "# Similar to fit, search will start training each model. It will only train for some epochs and then compare the results.\n",
    "tuner.search(train_ds, validation_data=valid_ds, epochs=epochs, verbose=verbose)\n",
    "\n",
    "# Gets the best optimiser and LR pair for use in subsequent models\n",
    "best_pair = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f'The best performing pair uses optimiser {best_pair.get('optimiser')} with learning rate {best_pair.get('learning_rate')}')"
   ],
   "id": "b9e9f80e08cf21b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To determine a more optimal number of epochs to use in cross-fold validation we will use an early stopping callback. This callback keeps track of the highest validation accuracy. If this value has not been beaten in 5 epochs, then we stop training and use the epoch number where we achieved the highest validation accuracy as the 'best' number of epochs to train with.",
   "id": "b04a0e026924acdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now we can train a model with the best performing optimiser & LR pair to find an optimal number of epochs to use\n",
    "# Build the model with the optimiser & LR\n",
    "googlenet = build_googlenet_model()\n",
    "# Instantiate the best performing optimiser & LR\n",
    "optimiser = make_optimiser(best_pair.get('optimiser'), best_pair.get('learning_rate'))\n",
    "# Compile our model with this optimiser\n",
    "googlenet.compile(optimizer=optimiser,\n",
    "                  loss={'output': 'categorical_crossentropy', 'aux1': 'categorical_crossentropy', 'aux2': 'categorical_crossentropy'},\n",
    "                  loss_weights={'output': 1.0, 'aux1': 0.3, 'aux2': 0.3},\n",
    "                  metrics=['accuracy', 'accuracy', 'accuracy'])\n",
    "\n",
    "# This callback will stop training if validation accuracy has not seen a new max in 5 epochs (patience)\n",
    "stop_early = EarlyStopping(monitor='val_output_accuracy', patience=5, verbose=1, mode='max')\n",
    "# Now we train with 50 epochs which is likely more than needed in order to see how many epochs we actually should train with\n",
    "history = googlenet.fit(train_ds, validation_data=valid_ds, epochs=epochs, batch_size=128, verbose=verbose, callbacks=[stop_early])"
   ],
   "id": "25b2e70ac0e923c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the validation accuracy for each epoch\n",
    "val_accuracy_by_epoch = history.history['val_output_accuracy']\n",
    "# Find the epoch number with the highest validation accuracy (+1 since arrays start at 0)\n",
    "# Reassign epochs as this is now the number we will use for future training\n",
    "epochs = val_accuracy_by_epoch.index(max(val_accuracy_by_epoch)) + 1\n",
    "print(f'The model had the best validation accuracy after {epochs} epochs')"
   ],
   "id": "8a5511dcad9b589",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cross-fold validation",
   "id": "5b8303a017ecea0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We will now retrain the model once more with the optimal epoch number\n",
    "full_train_ds = utils.image_dataset_from_directory(\n",
    "    directory=train_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size=input_shape, # Images are resized to the correct dimensions\n",
    "    pad_to_aspect_ratio=True, # Padding images so as not to lose valuable features & preserve aspect ratio\n",
    "    seed=123, # Use a seed for reproducibility\n",
    "    verbose=verbose)\n",
    "\n",
    "# Training models with Cross Fold Validation\n",
    "\n",
    "# Splits is the number of folds\n",
    "splits = 5\n",
    "# Total number of elements in the training data\n",
    "dataset_size = len(list(full_train_ds))\n",
    "# The number of elements in each fold\n",
    "fold_size = dataset_size // splits\n",
    "# The results of each fold will be stored in these arrays\n",
    "googlenet_scores = []\n",
    "googlenet_evaluation = []\n",
    "\n",
    "# For each fold we will split the training data into 3 parts, 2 for training and 1 for validation and repeat for each possible permutation\n",
    "for fold in range(splits):\n",
    "    # Create a new model\n",
    "    googlenet = build_googlenet_model()\n",
    "    # Re-instantiate the optimiser\n",
    "    optimiser = make_optimiser(best_pair.get('optimiser'), best_pair.get('learning_rate'))\n",
    "    googlenet.compile(optimizer=optimiser,\n",
    "                  loss={'output': 'categorical_crossentropy', 'aux1': 'categorical_crossentropy', 'aux2': 'categorical_crossentropy'},\n",
    "                  loss_weights={'output': 1.0, 'aux1': 0.3, 'aux2': 0.3},\n",
    "                  metrics=[['accuracy', 'precision', 'recall'], ['accuracy', 'precision', 'recall'], ['accuracy', 'precision', 'recall']])\n",
    "    # Train the model with the specific training sets and validation set\n",
    "    # Get the 'fold'th' third of the training data for validation (either the 1st, 2nd, or 3rd third)\n",
    "    valid_fold = full_train_ds.skip(fold * fold_size).take(fold_size)\n",
    "    # Get the remaining training data for training (gets the data after the validation split and concatenates the data before the validation split)\n",
    "    train_fold = full_train_ds.skip((fold+1) * fold_size).concatenate(full_train_ds.take(fold * fold_size))\n",
    "    \n",
    "    googlenet_scores.append(googlenet.fit(train_fold, epochs=epochs, validation_data=valid_fold, batch_size=128, verbose=verbose))\n",
    "    # Evaluate the model and store the results\n",
    "    googlenet_evaluation.append(googlenet.evaluate(X_test, y_test))"
   ],
   "id": "b3fa875bad21e67c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Results\n",
    "\n",
    "### GoogLeNet"
   ],
   "id": "5544b6b3bac39e38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d354807e00d04e6d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
